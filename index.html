<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Cathy  Jiao</title>
    <meta name="author" content="Cathy  Jiao">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://cathyjiao.github.io/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Cathy Jiao
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pic-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pic-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pic-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address">
              <br> <p>Email: cljiao@cs.cmu.edu </p>

            </div>
          </div>

          <div class="clearfix">
            <p>Hello! I am an incoming PhD student at the <a href="https://www.lti.cs.cmu.edu/" rel="external nofollow noopener" target="_blank">Language Technologies Institute</a> in the <a href="https://www.cs.cmu.edu/" rel="external nofollow noopener" target="_blank">School of Computer Science</a> at <a href="https://www.cs.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon University</a>.</p>

<p>I recently finished my masters degree at CMU LTI, where I was advised by <a href="https://www.cs.cmu.edu/~max/index.html" rel="external nofollow noopener" target="_blank">Maxine Eskenazi</a> and <a href="https://www.ri.cmu.edu/ri-faculty/aaron-steinfeld/" rel="external nofollow noopener" target="_blank">Aaron Steinfeld</a>. During my masters, I investigated generalization abilities in large language models – mostly in the realm of dialogue tasks [<a href="https://arxiv.org/abs/2205.12673" rel="external nofollow noopener" target="_blank">EMNLP ‘22</a>, <a href="https://embodied-ai.org/papers/2022/20.pdf" rel="external nofollow noopener" target="_blank">CVPR ‘22 (Embodied AI)</a>, <a href="https://arxiv.org/abs/2301.12004" rel="external nofollow noopener" target="_blank">IWSDS ‘23</a>].</p>

<p>Before grad school, I spent time in industry working on machine learning and deep learning applications for natural language processing. Prior to that, I graduated with distinction from the <a href="https://www.ubc.ca/" rel="external nofollow noopener" target="_blank">University of British Columbia</a> with a B.S. in <a href="https://www.cs.ubc.ca/" rel="external nofollow noopener" target="_blank">Computer 
Science</a> and <a href="https://www.ubc.ca/" rel="external nofollow noopener" target="_blank">Mathematics</a>.</p>

<p><a href="/assets/pdf/cv_cathy_jiao_2023.pdf">[CV]</a> <a href="https://www.semanticscholar.org/author/Cathy-Jiao/2064549240" rel="external nofollow noopener" target="_blank">[Semantic Scholar]</a> <a href="https://scholar.google.com/citations?user=fd1et9QAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">[Google Scholar]</a></p>

          </div>

          <!--<div class="news">
  <h2>News</h2>
  <div class="table-responsive">
    <table class="table table-sm table-borderless">
     
      <tr>
        <th scope="row">01/2023</th>
        <td>
          Our work on dialogue evalution with large language models was presented at IWSDS ‘23.
 
        </td>
      </tr> 
      <tr>
        <th scope="row">09/2022</th>
        <td>
          Demoed our DialPort Portal and Dashboard tool at SIGDIAL’ 22.
 
        </td>
      </tr> 
      <tr>
        <th scope="row">05/2022</th>
        <td>
          Presented our work on model generalization in embodied instruction following virtually at CVPR ‘22.
 
        </td>
      </tr> 
      <tr>
        <th scope="row">05/2022</th>
        <td>
          We released the <a href="https://github.com/prakharguptaz/Instructdial">code</a> for our EMNLP paper <a href="https://arxiv.org/abs/2205.12673">InstructDial</a> which introduces an instruction tuning framework for dialogue-related tasks.
 
        </td>
      </tr> 
      <tr>
        <th scope="row">03/2022</th>
        <td>
          Patent granted for my undergrad internship project on NLP for video search at Motorola Solutions.
 
        </td>
      </tr> 
    </table>
  </div> 
</div>-->
          <!-- Selected papers --><div class="publications">
  <h2>Publications</h2>
  <p><i>*= equal contribution </i></p>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <!--
        <div class="col-sm-2 abbr"></div>
         -->

        <!-- Entry bib key -->
        <div id="TNLG" class="col-sm-12">
        
          <!-- Title -->
          <div class="title">Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation</div>
          <!-- Author -->
          <div class="author">
          

          <!---->

          <!---->Jessica Huynh, <!---->
                  <em>Cathy Jiao</em>, <!---->Prakhar Gupta, <!---->Shikib Mehri, <!---->Payal Bajaj, <!---->Vishrav Chaudhary, <!----> and 
                Maxine Eskenazi

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            IWSDS 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btnabs btn-sm z-depth-0" role="button"><b>Abs</b></a>
            <a href="https://arxiv.org/abs/2301.12004" class="btnpdf btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><b>PDF</b></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Language models have steadily increased in size over the past few years. They achieve a high level of performance on various natural language processing (NLP) tasks such as question answering and summarization. Large language models (LLMs) have been used for generation and can now output human-like text. Due to this, there are other downstream tasks in the realm of dialog that can now harness the LLMs’ language understanding capabilities. Dialog evaluation is one task that this paper will explore. It concentrates on prompting with LLMs: BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured. Specifically, the more diverse and relevant the group of datasets that a model is trained on, the better dialog evaluation performs. This paper also investigates how the number of examples in the prompt and the type of example selection used affect the model’s performance.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <!--
        <div class="col-sm-2 abbr"></div>
         -->

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2205.12673" class="col-sm-12">
        
          <!-- Title -->
          <div class="title">Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning</div>
          <!-- Author -->
          <div class="author">
          

          <!---->

          <!---->Prakhar Gupta, <!---->
                  <em>Cathy Jiao</em>, <!---->Yi-Ting Yeh, <!---->Shikib Mehri, <!---->Maxine Eskenazi, <!----> and 
                Jeffrey P. Bigham

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            EMNLP 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btnabs btn-sm z-depth-0" role="button"><b>Abs</b></a>
            <a href="https://arxiv.org/abs/2205.12673" class="btnpdf btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><b>PDF</b></a>
            <a href="https://github.com/prakharguptaz/Instructdial" class="btncode btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><b>Code</b></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <!--
        <div class="col-sm-2 abbr"></div>
         -->

        <!-- Entry bib key -->
        <div id="CompAQT" class="col-sm-12">
        
          <!-- Title -->
          <div class="title">Improving compositional generalization for multi-step quantitative reasoning in question answering</div>
          <!-- Author -->
          <div class="author">
          

          <!---->

          <!---->Armineh Nourbakhsh, <!---->
                  <em>Cathy Jiao</em>, <!---->Sameena Shah, <!----> and 
                Carolyn Rosé

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            EMNLP 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btnabs btn-sm z-depth-0" role="button"><b>Abs</b></a>
            <a href="https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.125" class="btnpdf btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><b>PDF</b></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Quantitative reasoning is an important aspect of question answering, especially when numeric and verbal cues interact to indicate sophisticated, multi-step programs. In this paper, we demonstrate how modeling the compositional nature of quantitative text can enhance the performance and robustness of QA models, allowing them to capture arithmetic logic that is expressed verbally. Borrowing from the literature on semantic parsing, we propose a method that encourages the QA models to adjust their attention patterns and capture input/output alignments that are meaningful to the reasoning task. We show how this strategy improves program accuracy and renders the models more robust against overfitting as the number of reasoning steps grows. Our approach is designed as a standalone module which can be prepended to many existing models and trained in an end-to-end fashion without the need for additional supervisory signal. As part of this exercise, we also create a unified dataset building on four previously released numerical QA datasets over tabular data.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <!--
        <div class="col-sm-2 abbr"></div>
         -->

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2208.10918" class="col-sm-12">
        
          <!-- Title -->
          <div class="title">The DialPort tools</div>
          <!-- Author -->
          <div class="author">
          

          <!---->

          <!---->Jessica Huynh*, <!---->Shikib Mehri*, <!---->
                  <em>Cathy Jiao*</em>, <!----> and 
                Maxine Eskenazi

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            SIGDIAL 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btnabs btn-sm z-depth-0" role="button"><b>Abs</b></a>
            <a href="https://arxiv.org/abs/2208.10918" class="btnpdf btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><b>PDF</b></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The DialPort project (http://dialport.org/), funded by the National Science Foundation (NSF), covers a group of tools and services that aim at fulfilling the needs of the dialog research community. Over the course of six years, several offerings have been created, including the DialPort Portal and DialCrowd. This paper describes these contributions, which will be demoed at SIGDIAL, including implementation, prior studies, corresponding discoveries, and the locations at which the tools will remain freely available to the community going forward.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <!--
        <div class="col-sm-2 abbr"></div>
         -->

        <!-- Entry bib key -->
        <div id="byuntu" class="col-sm-12">
        
          <!-- Title -->
          <div class="title">ET tu, CLIP? Addressing Common Object Errors for Unseen Environments</div>
          <!-- Author -->
          <div class="author">
          

          <!---->

          <!---->Ye Won Byun*, <!---->
                  <em>Cathy Jiao*</em>, <!---->Shahriar Noroozizadeh*, <!---->Jimin Sun*, <!----> and 
                Rosa Vitiello*

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em></em> CVPR, Embodied AI Workshop, 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btnabs btn-sm z-depth-0" role="button"><b>Abs</b></a>
            <a href="https://embodied-ai.org/papers/2022/20.pdf" class="btnpdf btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><b>PDF</b></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce a simple method that employs pre-trained CLIP encoders to enhance model generalization in the ALFRED task. In contrast to previous literature where CLIP replaces the visual encoder, we suggest using CLIP as an additional module through an auxiliary object detection objective. We validate our method on the recently proposed Episodic Transformer architecture and demonstrate that incorporating CLIP improves task performance on the unseen validation set. Additionally, our analysis results support that CLIP especially helps with leveraging object descriptions, detecting small objects, and interpreting rare words.</p>
          </div>
        </div>
      </div>
</li>
</ol>
</div>
<div class="publications">
    <h2>Patents</h2>
    <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <!--
        <div class="col-sm-2 abbr"></div>
         -->

        <!-- Entry bib key -->
        <div id="name" class="col-sm-12">
        
          <!-- Title -->
          <div class="title">Natural language text conversion and method therefor</div>
          <!-- Author -->
          <div class="author">
          

          <!---->

          <!---->Roger Donaldson, <!----> and 
                <em>Cathy Jiao</em>
                

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            U.S. Patent 11275906, 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btnabs btn-sm z-depth-0" role="button"><b>Abs</b></a>
            <a href="https://patents.google.com/patent/US20210019374A1" class="btnpdf btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><b>PDF</b></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multiple natural language training text strings are obtained. For example, text portions may be randomly selected and converted into natural language text based on one or more randomly selected rules. A formatted training text string is generated for each natural language training text string, for example using a context-free grammar parser. The formatted training text strings are inputted to a machine learning model. For each formatted training text string, using the machine learning model, a natural language text string is generated. The natural language text string is associated with one of the natural language training text strings. One or more parameters of the machine learning model are adjusted based on one or more differences between at least one of the natural language text strings and its associated natural language training text string.</p>
          </div>
        </div>
      </div>
</li></ol>
</div>
<div>
    <h2>Service</h2>
    <p>Reviewer: ACL '23, EMNLP '22</p>
</div>
        </article>

</div>
    </div>

    <!-- Footer    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2023 Cathy  Jiao. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

      </div>
    </footer> -->

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
