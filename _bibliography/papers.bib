---
---

@misc{TNLG,
  author = {Huynh, Jessica and Jiao, Cathy and Gupta, Prakhar and Mehri, Shikib and Bajaj,Payal and Chaudhary, Vishrav and Eskenazi, Maxine },
  title = {Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation},
  year = {IWSDS 2023},
  pdf={https://arxiv.org/abs/2301.12004},
  selected={true},
  abstract={Language models have steadily increased in size over the past few years. They achieve a high level of performance on various natural language processing (NLP) tasks such as question answering and summarization. Large language models (LLMs) have been used for generation and can now output human-like text. Due to this, there are other downstream tasks in the realm of dialog that can now harness the LLMs' language understanding capabilities. Dialog evaluation is one task that this paper will explore. It concentrates on prompting with LLMs: BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured. Specifically, the more diverse and relevant the group of datasets that a model is trained on, the better dialog evaluation performs. This paper also investigates how the number of examples in the prompt and the type of example selection used affect the model's performance.}
}

@misc{https://doi.org/10.48550/arxiv.2205.12673,
  doi = {10.48550/ARXIV.2205.12673},
  url = {https://arxiv.org/abs/2205.12673},
  author = {Gupta, Prakhar and Jiao, Cathy and Yeh, Yi-Ting and Mehri, Shikib and Eskenazi, Maxine and Bigham, Jeffrey P.},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning},
  publisher = {arXiv},
  year = {EMNLP 2022},
  copyright = {Creative Commons Attribution 4.0 International},
  code = {https://github.com/prakharguptaz/Instructdial},
  pdf={https://arxiv.org/abs/2205.12673},
  selected={true},
  abstract={Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.}
}

@misc{CompAQT,
  author = {Nourbakhsh, Armineh and Jiao, Cathy and Shah, Sameena and Ros√©, Carolyn},
  title = {Improving compositional generalization for multi-step quantitative reasoning in question answering},
  year = {EMNLP 2022},
  pdf={https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.125},
  selected={true},
  abstract={Quantitative reasoning is an important aspect of question answering, especially when numeric and verbal cues interact to indicate sophisticated, multi-step programs. In this paper, we demonstrate how modeling the compositional nature of quantitative text can enhance the performance and robustness of QA models, allowing them to capture arithmetic logic that is expressed verbally. Borrowing from the literature on semantic parsing, we propose a method that encourages the QA models to adjust their attention patterns and capture input/output alignments that are meaningful to the reasoning task. We show how this strategy improves program accuracy and renders the models more robust against overfitting as the number of reasoning steps grows. Our approach is designed as a standalone module which can be prepended to many existing models and trained in an end-to-end fashion without the need for additional supervisory signal. As part of this exercise, we also create a unified dataset building on four previously released numerical QA datasets over tabular data.}
}

@misc{https://doi.org/10.48550/arxiv.2208.10918,
  author = {Huynh*, Jessica and Mehri*, Shikib and Jiao*, Cathy and Eskenazi, Maxine},
  title = {The DialPort tools},
  year = {SIGDIAL 2022},
  selected={true},
  pdf={https://arxiv.org/abs/2208.10918},
  abstract={The DialPort project (http://dialport.org/), funded by the National Science Foundation (NSF), covers a group of tools and services that aim at fulfilling the needs of the dialog research community. Over the course of six years, several offerings have been created, including the DialPort Portal and DialCrowd. This paper describes these contributions, which will be demoed at SIGDIAL, including implementation, prior studies, corresponding discoveries, and the locations at which the tools will remain freely available to the community going forward.}
}



@article{byuntu,
  title={ET tu, CLIP? Addressing Common Object Errors for Unseen Environments},
  author={Byun*, Ye Won and Jiao*, Cathy and Noroozizadeh*, Shahriar and Sun*, Jimin and Vitiello*, Rosa},
  year = {CVPR, Embodied AI Workshop, 2022},
  pdf={https://embodied-ai.org/papers/2022/20.pdf},
  selected={true},
  abstract={We introduce a simple method that employs pre-trained CLIP encoders to enhance model generalization in the ALFRED task. In contrast to previous literature where CLIP replaces the visual encoder, we suggest using CLIP as an additional module through an auxiliary object detection objective. We validate our method on the recently proposed Episodic Transformer architecture and demonstrate that incorporating CLIP improves task performance on the unseen validation set. Additionally, our analysis results support that CLIP especially helps with leveraging object descriptions, detecting small objects, and interpreting rare words.}
}

@misc{name,
 title={Natural language text conversion and method therefor},
 author={Donaldson, Roger and Jiao, Cathy},
 year={U.S. Patent 11275906, 2022},
 patent={true},
 pdf={https://patents.google.com/patent/US20210019374A1},
 abstract={Multiple natural language training text strings are obtained. For example, text portions may be randomly selected and converted into natural language text based on one or more randomly selected rules. A formatted training text string is generated for each natural language training text string, for example using a context-free grammar parser. The formatted training text strings are inputted to a machine learning model. For each formatted training text string, using the machine learning model, a natural language text string is generated. The natural language text string is associated with one of the natural language training text strings. One or more parameters of the machine learning model are adjusted based on one or more differences between at least one of the natural language text strings and its associated natural language training text string.}
 }
